Azure Data Engineering Pipeline
ğŸ“Œ Project Overview

This project demonstrates an end-to-end data engineering pipeline built on Azure Data Lake Storage Gen2 (ADLS) using Azure Data Factory (ADF), PySpark, and Power BI.

The pipeline ingests raw data from CSV files and the GitHub API, applies transformations using the Medallion Architecture (Bronze â†’ Silver â†’ Gold layers), and visualizes key insights in Power BI.

The solution is designed with performance in mind, featuring incremental loading and watermarking techniques to reduce data latency and optimize pipeline efficiency.

ğŸ— Architecture

Data Ingestion (Bronze Layer)

Extracted data from CSV files and GitHub API.

Loaded data into Azure Data Lake Gen2 using Azure Data Factory.

Data Transformation (Silver Layer)

Applied data cleaning, filtering, and schema normalization with PySpark.

Implemented incremental loading with watermarking for efficiency.

Data Enrichment & Aggregation (Gold Layer)

Curated analytical datasets for reporting.

Designed star-schema-like structures for ease of visualization.

Visualization

Built interactive dashboards in Power BI to deliver actionable insights.

ğŸš€ Key Features

âœ… Automated ingestion from multiple sources (CSV, API)

âœ… Incremental data loading with watermarking

âœ… Data processing with Medallion Architecture (Bronze, Silver, Gold)

âœ… Data transformation & enrichment using PySpark

âœ… Insightful Power BI dashboards

âœ… Reduced data latency by 25%

ğŸ› ï¸ Technologies Used

Azure Data Factory â€“ Orchestration & Ingestion

Azure Data Lake Storage Gen2 â€“ Data Storage

PySpark â€“ Data Processing & Transformations

Power BI â€“ Visualization & Analytics

ğŸ“Š Outcomes

Reduced data latency by 25%.

Optimized pipeline performance through incremental loads.

Delivered a scalable data analytics solution using Azure services.
